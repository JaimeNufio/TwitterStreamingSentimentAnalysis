{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae16052e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import re, string\n",
    "\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, CountVectorizer, IDF, HashingTF, StringIndexer, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "from pyspark.ml.classification import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fea18b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf().setMaster(\"local[7]\").setAppName(\"ModelTrain\")\n",
    "sc = SparkContext(conf = conf)\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "359968a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load sample data to train/test on\n",
    "\n",
    "input = sc.textFile(\"./train.csv\")\n",
    "dataTrain = ( input.map(lambda x: (x.split('\",\"')[0][1],x.split('\",\"')[5])) # get sentiment and text\n",
    "    .map(lambda x: (x[0],re.sub(r'@[^\\s]+','',x[1])))  #remove mentions\n",
    "    .map(lambda x: (x[0],re.sub(r\"\\S*http?:\\S*\",'',x[1])))  #remove urls\n",
    "    .map(lambda x: (x[0],re.sub(r'[^A-Za-z0-9 ]','',x[1])))  #remove special\n",
    "    .map(lambda x: (x[0],re.sub(r'[ ]{2,}',' ',x[1])))  #remove double+ spaces\n",
    "    .map(lambda x: (x[0],x[1].strip().lower())) #strip space, force to lowercase\n",
    "    .map(lambda x: (x[0],x[1]))\n",
    "       )\n",
    "\n",
    "input = sc.textFile(\"./test.csv\")\n",
    "dataTest = ( input.map(lambda x: (x.split('\",\"')[0][1],x.split('\",\"')[5])) # get sentiment and text\n",
    "    .map(lambda x: (x[0],re.sub(r'@[^\\s]+','',x[1])))  #remove mentions\n",
    "    .map(lambda x: (x[0],re.sub(r\"\\S*http?:\\S*\",'',x[1])))  #remove urls\n",
    "    .map(lambda x: (x[0],re.sub(r'[^A-Za-z0-9 ]','',x[1])))  #remove special\n",
    "    .map(lambda x: (x[0],re.sub(r'[ ]{2,}',' ',x[1])))  #remove double+ spaces\n",
    "    .map(lambda x: (x[0],x[1].strip().lower())) #strip space, force to lowercase\n",
    "    .map(lambda x: (x[0],x[1]))\n",
    "       )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f44ce1b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = dataTrain.toDF(['sentiment','text'])\n",
    "dfTrain = dataTrain.toDF(['sentiment','text'])\n",
    "dfTest = dataTest.toDF(['sentiment','text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "173d7ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer().setInputCol('text').setOutputCol('words')\n",
    "remover = StopWordsRemover(inputCol='words',outputCol='clean')\n",
    "TF = HashingTF(numFeatures=2**16,inputCol=\"clean\",outputCol='tf')\n",
    "IDF = IDF(inputCol='tf',outputCol=\"features\")\n",
    "StringIndex = StringIndexer(inputCol=\"sentiment\",outputCol=\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b961e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Model = LogisticRegression(maxIter=300)\n",
    "pipeline = Pipeline(stages=[tokenizer,remover,TF,IDF,StringIndex,Model])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db37dcbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelineFit = pipeline.fit(dfTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d8ca55a0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "predict = pipelineFit.transform(dfTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fcad79fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "27eeafa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n",
      "|                text|prediction|\n",
      "+--------------------+----------+\n",
      "|i loooooooovvvvvv...|       1.0|\n",
      "|reading my kindle...|       1.0|\n",
      "|ok first assesmen...|       1.0|\n",
      "|youll love your k...|       1.0|\n",
      "|fair enough but i...|       1.0|\n",
      "|no it is too big ...|       1.0|\n",
      "|fuck this economy...|       0.0|\n",
      "|jquery is my new ...|       1.0|\n",
      "|       loves twitter|       1.0|\n",
      "|how can you not l...|       1.0|\n",
      "|check this video ...|       1.0|\n",
      "|i firmly believe ...|       0.0|\n",
      "|house corresponde...|       0.0|\n",
      "|watchin espnjus s...|       0.0|\n",
      "|dear nike stop wi...|       0.0|\n",
      "|lebron best athle...|       0.0|\n",
      "|i was talking to ...|       0.0|\n",
      "|       i love lebron|       1.0|\n",
      "|lebron is a beast...|       0.0|\n",
      "|  lebron is the boss|       1.0|\n",
      "|lebron is a homet...|       1.0|\n",
      "|lebron and zydrun...|       1.0|\n",
      "|lebron is a beast...|       0.0|\n",
      "|downloading apps ...|       0.0|\n",
      "|good news just ha...|       1.0|\n",
      "|awesome come back...|       1.0|\n",
      "|in montreal for a...|       1.0|\n",
      "|booz allen hamilt...|       1.0|\n",
      "|mluc09 customer i...|       1.0|\n",
      "|i current use the...|       1.0|\n",
      "|need suggestions ...|       1.0|\n",
      "|i just checked my...|       1.0|\n",
      "|google is always ...|       1.0|\n",
      "|played with an an...|       0.0|\n",
      "|us planning to re...|       1.0|\n",
      "|omg so bored amp ...|       0.0|\n",
      "|im itchy and mise...|       0.0|\n",
      "|no im not itchy f...|       0.0|\n",
      "|rt i love the ner...|       0.0|\n",
      "|has been a bit cr...|       1.0|\n",
      "|im listening to p...|       1.0|\n",
      "|is going to sleep...|       1.0|\n",
      "|cant sleep my too...|       0.0|\n",
      "|blah blah blah sa...|       0.0|\n",
      "|glad i didnt do b...|       0.0|\n",
      "|is in san francis...|       1.0|\n",
      "|just landed at sa...|       1.0|\n",
      "|san francisco tod...|       1.0|\n",
      "|obama administrat...|       1.0|\n",
      "|started to think ...|       1.0|\n",
      "|shaunwoo haten on...|       0.0|\n",
      "|you will not regr...|       1.0|\n",
      "|on my way to see ...|       1.0|\n",
      "|going to see star...|       1.0|\n",
      "|annoying new tren...|       0.0|\n",
      "|bill simmons in c...|       1.0|\n",
      "|highly recommend ...|       1.0|\n",
      "|blink by malcolm ...|       1.0|\n",
      "|malcolm gladwell ...|       1.0|\n",
      "|omg the commercia...|       0.0|\n",
      "|playing with twit...|       1.0|\n",
      "|playing with curl...|       1.0|\n",
      "|   hello twitter api|       1.0|\n",
      "|playing with java...|       1.0|\n",
      "|because the twitt...|       0.0|\n",
      "|yahoo answers can...|       1.0|\n",
      "|is scrapbooking w...|       1.0|\n",
      "|rt five things wo...|       1.0|\n",
      "|just changed my d...|       1.0|\n",
      "|nike owns nba pla...|       1.0|\n",
      "|next time ill cal...|       1.0|\n",
      "|new blog post nik...|       1.0|\n",
      "|rt was just told ...|       0.0|\n",
      "|back when i worke...|       1.0|\n",
      "|by the way im tot...|       1.0|\n",
      "|giving weka an ap...|       1.0|\n",
      "|brand new canon e...|       1.0|\n",
      "|class the 50d is ...|       1.0|\n",
      "|needs someone to ...|       0.0|\n",
      "|took the graduate...|       0.0|\n",
      "|shout outs to all...|       1.0|\n",
      "|yeahhhhhhhhh i wo...|       1.0|\n",
      "|great stanford co...|       1.0|\n",
      "|nvidia names stan...|       1.0|\n",
      "|new blog post har...|       1.0|\n",
      "|work til 6pm lets...|       1.0|\n",
      "|damn you north korea|       0.0|\n",
      "|can we just go ah...|       1.0|\n",
      "|north korea pleas...|       0.0|\n",
      "|why the hell is p...|       0.0|\n",
      "|are you burning m...|       1.0|\n",
      "|insects have infe...|       0.0|\n",
      "|wish i could catc...|       0.0|\n",
      "|just got back fro...|       0.0|\n",
      "|just got mcdonald...|       0.0|\n",
      "|omgg i ohhdee wan...|       0.0|\n",
      "|history exam stud...|       0.0|\n",
      "|i hate revision i...|       0.0|\n",
      "|higher physics ex...|       0.0|\n",
      "|its a bank holida...|       0.0|\n",
      "+--------------------+----------+\n",
      "only showing top 100 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predict.select(\"text\",\"prediction\").show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "25cc7067",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelineFit.write().overwrite().save(\"./Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ae441e77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.ml.pipeline.PipelineModel"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(pipelineFit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0cef603",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
