{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a021f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "import mllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "739809b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf().setMaster(\"local\").setAppName(\"ModelTrain\")\n",
    "sc = SparkContext(conf = conf)\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1447bce1",
   "metadata": {},
   "source": [
    "### Start Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3fc8aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "844b405d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load sample data to train/test on\n",
    "\n",
    "input = sc.textFile(\"./train.csv\")\n",
    "dataTrain = ( input.map(lambda x: (x.split('\",\"')[0][1],x.split('\",\"')[5])) # get sentiment and text\n",
    "    .map(lambda x: (x[0],re.sub(r'@[^\\s]+','',x[1])))  #remove mentions\n",
    "    .map(lambda x: (x[0],re.sub(r\"\\S*http?:\\S*\",'',x[1])))  #remove urls\n",
    "    .map(lambda x: (x[0],re.sub(r'[^A-Za-z0-9 ]','',x[1])))  #remove special\n",
    "    .map(lambda x: (x[0],re.sub(r'[ ]{2,}',' ',x[1])))  #remove double+ spaces\n",
    "    .map(lambda x: (x[0],x[1].strip().lower())) #strip space, force to lowercase\n",
    "    .map(lambda x: (x[0],x[1]))\n",
    "       )\n",
    "\n",
    "input = sc.textFile(\"./test.csv\")\n",
    "dataTest = ( input.map(lambda x: (x.split('\",\"')[0][1],x.split('\",\"')[5])) # get sentiment and text\n",
    "    .map(lambda x: (x[0],re.sub(r'@[^\\s]+','',x[1])))  #remove mentions\n",
    "    .map(lambda x: (x[0],re.sub(r\"\\S*http?:\\S*\",'',x[1])))  #remove urls\n",
    "    .map(lambda x: (x[0],re.sub(r'[^A-Za-z0-9 ]','',x[1])))  #remove special\n",
    "    .map(lambda x: (x[0],re.sub(r'[ ]{2,}',' ',x[1])))  #remove double+ spaces\n",
    "    .map(lambda x: (x[0],x[1].strip().lower())) #strip space, force to lowercase\n",
    "    .map(lambda x: (x[0],x[1]))\n",
    "       )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ceca6b4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------------------------------------------------------------------------------------------+\n",
      "|_1 |_2                                                                                                      |\n",
      "+---+--------------------------------------------------------------------------------------------------------+\n",
      "|0  |awww thats a bummer you shoulda got david carr of third day to do it d                                  |\n",
      "|0  |is upset that he cant update his facebook by texting it and might cry as a result school today also blah|\n",
      "|0  |i dived many times for the ball managed to save 50 the rest go out of bounds                            |\n",
      "|0  |my whole body feels itchy and like its on fire                                                          |\n",
      "|0  |no its not behaving at all im mad why am i here because i cant see you all over there                   |\n",
      "|0  |not the whole crew                                                                                      |\n",
      "|0  |need a hug                                                                                              |\n",
      "|0  |hey long time no see yes rains a bit only a bit lol im fine thanks hows you                             |\n",
      "|0  |nope they didnt have it                                                                                 |\n",
      "|0  |que me muera                                                                                            |\n",
      "|0  |spring break in plain city its snowing                                                                  |\n",
      "|0  |i just repierced my ears                                                                                |\n",
      "|0  |i couldnt bear to watch it and i thought the ua loss was embarrassing                                   |\n",
      "|0  |it it counts idk why i did either you never talk to me anymore                                          |\n",
      "|0  |i wouldve been the first but i didnt have a gun not really though zac snyders just a doucheclown        |\n",
      "|0  |i wish i got to watch it with you i miss you and how was the premiere                                   |\n",
      "|0  |hollis death scene will hurt me severely to watch on film wry is directors cut not out now              |\n",
      "|0  |about to file taxes                                                                                     |\n",
      "|0  |ahh ive always wanted to see rent love the soundtrack                                                   |\n",
      "|0  |oh dear were you drinking out of the forgotten table drinks                                             |\n",
      "+---+--------------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#delete this one later...\n",
    "\n",
    "df=dataTrain.toDF()\n",
    "dfTrain=dataTrain.toDF()\n",
    "dfTest=dataTest.toDF()\n",
    "\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61bb0751",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load into DataFrame\n",
    "df = dataTrain.toDF(['Sentiment','Text'])\n",
    "dfTest = dataTrain.toDF(['Sentiment','Text'])\n",
    "dfTrain = dataTrain.toDF(['Sentiment','Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2c5caed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+\n",
      "|Sentiment|                Text|\n",
      "+---------+--------------------+\n",
      "|        0|awww thats a bumm...|\n",
      "|        0|is upset that he ...|\n",
      "+---------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42043c81",
   "metadata": {},
   "source": [
    "## Natural Language Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab97971a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, CountVectorizer, IDF, HashingTF, StringIndexer\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dbcab1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seperate into words, remove stop words\n",
    "tokenizer = Tokenizer().setInputCol('Text').setOutputCol('Words')\n",
    "remover = StopWordsRemover(inputCol='Words',outputCol='Clean')\n",
    "\n",
    "df = tokenizer.transform(df)\n",
    "df = remover.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5ef4d846",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TF-IDF (Apparently goes well with logistic regression)\n",
    "TF = HashingTF(numFeatures=2**16,inputCol=\"Clean\",outputCol='TF')\n",
    "IDF = IDF(inputCol='TF',outputCol=\"features\")\n",
    "StringIndex = StringIndexer(inputCol=\"Sentiment\",outputCol=\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "83ddcbc1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "Model = LogisticRegression(maxIter=300)\n",
    "pipeline = Pipeline(stages=[tokenizer,remover,TF,IDF,StringIndex,Model])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ed06acbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelineFit = pipeline.fit(dfTrain)\n",
    "trainDF = pipelineFit.transform(dfTrain)\n",
    "valDF = pipelineFit.transform(dfTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cf043b88",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+----------+----------+----------+----------+-----+-------------+-----------+----------+\n",
      "|Sentiment|      Text|     Words|     Clean|        TF|  features|label|rawPrediction|probability|prediction|\n",
      "+---------+----------+----------+----------+----------+----------+-----+-------------+-----------+----------+\n",
      "|        0|awww th...|[awww, ...|[awww, ...|(65536,...|(65536,...|  0.0|   [3.0673...| [0.9555...|       0.0|\n",
      "|        0|is upse...|[is, up...|[upset,...|(65536,...|(65536,...|  0.0|   [5.3398...| [0.9952...|       0.0|\n",
      "|        0|i dived...|[i, div...|[dived,...|(65536,...|(65536,...|  0.0|   [1.7537...| [0.8524...|       0.0|\n",
      "+---------+----------+----------+----------+----------+----------+-----+-------------+-----------+----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainDF.show(3,truncate=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75e57c3",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6bc3358d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.795586875\n",
      "AUC:0.8721028877101562\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "accuracy = trainDF.filter(trainDF.label == trainDF.prediction).count()/float(trainDF.count())\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\n",
    "AUC = evaluator.evaluate(trainDF)\n",
    "print(\"Accuracy: {}\\nAUC:{}\".format(accuracy,AUC))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59fb5a7",
   "metadata": {},
   "source": [
    "Good enough for me!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8bf82d40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.795586875\n",
      "AUC:0.872101918366406\n"
     ]
    }
   ],
   "source": [
    "accuracy = valDF.filter(valDF.label == valDF.prediction).count()/float(valDF.count())\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\n",
    "AUC = evaluator.evaluate(valDF)\n",
    "print(\"Accuracy: {}\\nAUC:{}\".format(accuracy,AUC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e23bec1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
