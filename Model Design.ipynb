{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a021f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "import mllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "739809b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf().setMaster(\"local\").setAppName(\"ModelTrain\")\n",
    "sc = SparkContext(conf = conf)\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1447bce1",
   "metadata": {},
   "source": [
    "### Start Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3fc8aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "844b405d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load sample data to train/test on\n",
    "\n",
    "input = sc.textFile(\"./train.csv\")\n",
    "dataTrain = ( input.map(lambda x: (x.split('\",\"')[0][1],x.split('\",\"')[5])) # get sentiment and text\n",
    "    .map(lambda x: (x[0],re.sub(r'@[^\\s]+','',x[1])))  #remove mentions\n",
    "    .map(lambda x: (x[0],re.sub(r\"\\S*http?:\\S*\",'',x[1])))  #remove urls\n",
    "    .map(lambda x: (x[0],re.sub(r'[^A-Za-z0-9 ]','',x[1])))  #remove special\n",
    "    .map(lambda x: (x[0],re.sub(r'[ ]{2,}',' ',x[1])))  #remove double+ spaces\n",
    "    .map(lambda x: (x[0],x[1].strip().lower())) #strip space, force to lowercase\n",
    "    .map(lambda x: (x[0],x[1]))\n",
    "       )\n",
    "\n",
    "input = sc.textFile(\"./test.csv\")\n",
    "dataTest = ( input.map(lambda x: (x.split('\",\"')[0][1],x.split('\",\"')[5])) # get sentiment and text\n",
    "    .map(lambda x: (x[0],re.sub(r'@[^\\s]+','',x[1])))  #remove mentions\n",
    "    .map(lambda x: (x[0],re.sub(r\"\\S*http?:\\S*\",'',x[1])))  #remove urls\n",
    "    .map(lambda x: (x[0],re.sub(r'[^A-Za-z0-9 ]','',x[1])))  #remove special\n",
    "    .map(lambda x: (x[0],re.sub(r'[ ]{2,}',' ',x[1])))  #remove double+ spaces\n",
    "    .map(lambda x: (x[0],x[1].strip().lower())) #strip space, force to lowercase\n",
    "    .map(lambda x: (x[0],x[1]))\n",
    "       )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ceca6b4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------------------------------------------------------------------------------------------+\n",
      "|_1 |_2                                                                                                      |\n",
      "+---+--------------------------------------------------------------------------------------------------------+\n",
      "|0  |awww thats a bummer you shoulda got david carr of third day to do it d                                  |\n",
      "|0  |is upset that he cant update his facebook by texting it and might cry as a result school today also blah|\n",
      "|0  |i dived many times for the ball managed to save 50 the rest go out of bounds                            |\n",
      "|0  |my whole body feels itchy and like its on fire                                                          |\n",
      "|0  |no its not behaving at all im mad why am i here because i cant see you all over there                   |\n",
      "|0  |not the whole crew                                                                                      |\n",
      "|0  |need a hug                                                                                              |\n",
      "|0  |hey long time no see yes rains a bit only a bit lol im fine thanks hows you                             |\n",
      "|0  |nope they didnt have it                                                                                 |\n",
      "|0  |que me muera                                                                                            |\n",
      "|0  |spring break in plain city its snowing                                                                  |\n",
      "|0  |i just repierced my ears                                                                                |\n",
      "|0  |i couldnt bear to watch it and i thought the ua loss was embarrassing                                   |\n",
      "|0  |it it counts idk why i did either you never talk to me anymore                                          |\n",
      "|0  |i wouldve been the first but i didnt have a gun not really though zac snyders just a doucheclown        |\n",
      "|0  |i wish i got to watch it with you i miss you and how was the premiere                                   |\n",
      "|0  |hollis death scene will hurt me severely to watch on film wry is directors cut not out now              |\n",
      "|0  |about to file taxes                                                                                     |\n",
      "|0  |ahh ive always wanted to see rent love the soundtrack                                                   |\n",
      "|0  |oh dear were you drinking out of the forgotten table drinks                                             |\n",
      "+---+--------------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#delete this one later...\n",
    "\n",
    "df=dataTrain.toDF()\n",
    "dfTrain=dataTrain.toDF()\n",
    "dfTest=dataTest.toDF()\n",
    "\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61bb0751",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load into DataFrame\n",
    "df = dataTrain.toDF(['Sentiment','Text'])\n",
    "dfTest = dataTrain.toDF(['Sentiment','Text'])\n",
    "dfTrain = dataTrain.toDF(['Sentiment','Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2c5caed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+\n",
      "|Sentiment|                Text|\n",
      "+---------+--------------------+\n",
      "|        0|awww thats a bumm...|\n",
      "|        0|is upset that he ...|\n",
      "+---------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42043c81",
   "metadata": {},
   "source": [
    "## Natural Language Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab97971a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, CountVectorizer,IDF, HashingTF, StringIndexer, VectorAssembler\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "25a5c6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assembler = VectorAssembler().transform(df).setInputCols('Text').setOutputCols('Sentiment')\n",
    "# df = Assembler.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "41c649b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+\n",
      "|Sentiment|                Text|\n",
      "+---------+--------------------+\n",
      "|        0|awww thats a bumm...|\n",
      "|        0|is upset that he ...|\n",
      "|        0|i dived many time...|\n",
      "|        0|my whole body fee...|\n",
      "|        0|no its not behavi...|\n",
      "|        0|  not the whole crew|\n",
      "|        0|          need a hug|\n",
      "|        0|hey long time no ...|\n",
      "|        0|nope they didnt h...|\n",
      "|        0|        que me muera|\n",
      "|        0|spring break in p...|\n",
      "|        0|i just repierced ...|\n",
      "|        0|i couldnt bear to...|\n",
      "|        0|it it counts idk ...|\n",
      "|        0|i wouldve been th...|\n",
      "|        0|i wish i got to w...|\n",
      "|        0|hollis death scen...|\n",
      "|        0| about to file taxes|\n",
      "|        0|ahh ive always wa...|\n",
      "|        0|oh dear were you ...|\n",
      "+---------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dbcab1d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+--------------------+--------------------+\n",
      "|Sentiment|                Text|               Words|               Clean|\n",
      "+---------+--------------------+--------------------+--------------------+\n",
      "|        0|awww thats a bumm...|[awww, thats, a, ...|[awww, thats, bum...|\n",
      "|        0|is upset that he ...|[is, upset, that,...|[upset, cant, upd...|\n",
      "|        0|i dived many time...|[i, dived, many, ...|[dived, many, tim...|\n",
      "|        0|my whole body fee...|[my, whole, body,...|[whole, body, fee...|\n",
      "|        0|no its not behavi...|[no, its, not, be...|[behaving, im, ma...|\n",
      "|        0|  not the whole crew|[not, the, whole,...|       [whole, crew]|\n",
      "|        0|          need a hug|      [need, a, hug]|         [need, hug]|\n",
      "|        0|hey long time no ...|[hey, long, time,...|[hey, long, time,...|\n",
      "|        0|nope they didnt h...|[nope, they, didn...|       [nope, didnt]|\n",
      "|        0|        que me muera|    [que, me, muera]|        [que, muera]|\n",
      "|        0|spring break in p...|[spring, break, i...|[spring, break, p...|\n",
      "|        0|i just repierced ...|[i, just, repierc...|   [repierced, ears]|\n",
      "|        0|i couldnt bear to...|[i, couldnt, bear...|[couldnt, bear, w...|\n",
      "|        0|it it counts idk ...|[it, it, counts, ...|[counts, idk, eit...|\n",
      "|        0|i wouldve been th...|[i, wouldve, been...|[wouldve, first, ...|\n",
      "|        0|i wish i got to w...|[i, wish, i, got,...|[wish, got, watch...|\n",
      "|        0|hollis death scen...|[hollis, death, s...|[hollis, death, s...|\n",
      "|        0| about to file taxes|[about, to, file,...|       [file, taxes]|\n",
      "|        0|ahh ive always wa...|[ahh, ive, always...|[ahh, ive, always...|\n",
      "|        0|oh dear were you ...|[oh, dear, were, ...|[oh, dear, drinki...|\n",
      "+---------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Seperate into words, remove stop words\n",
    "tokenizer = Tokenizer().setInputCol('Text').setOutputCol('Words')\n",
    "remover = StopWordsRemover(inputCol='Words',outputCol='Clean')\n",
    "\n",
    "df = tokenizer.transform(df)\n",
    "df = remover.transform(df)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5ef4d846",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TF-IDF (Apparently goes well with logistic regression)\n",
    "TF = HashingTF(numFeatures=2**16,inputCol=\"Clean\",outputCol='TF')\n",
    "IDF = IDF(inputCol='TF',outputCol=\"features\")\n",
    "StringIndex = StringIndexer(inputCol=\"Sentiment\",outputCol=\"label\")\n",
    "\n",
    "# df = TF.fit(df)\n",
    "# df = IDF.fit(df)\n",
    "df = StringIndex.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "83ddcbc1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "Model = LogisticRegression(maxIter=300)\n",
    "pipeline = Pipeline(stages=[tokenizer,remover,TF,IDF,StringIndex,Model])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ed06acbd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pipelineFit = pipeline.fit(dfTrain)\n",
    "trainDF = pipelineFit.transform(dfTrain)\n",
    "#valDF = pipelineFit.transform(dfTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cf043b88",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#trainDF.show(3,truncate=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75e57c3",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6bc3358d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-36736ac28ba7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBinaryClassificationEvaluator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainDF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainDF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtrainDF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainDF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mevaluator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBinaryClassificationEvaluator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrawPredictionCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"rawPrediction\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mAUC\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainDF\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mcount\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    662\u001b[0m         \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    663\u001b[0m         \"\"\"\n\u001b[0;32m--> 664\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1301\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1303\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1031\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1032\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1033\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1034\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1035\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1199\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1200\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1201\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1202\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    667\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    668\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 669\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    670\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "accuracy = trainDF.filter(trainDF.label == trainDF.prediction).count()/float(trainDF.count())\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\n",
    "AUC = evaluator.evaluate(trainDF)\n",
    "print(\"Accuracy: {}\\nAUC:{}\".format(accuracy,AUC))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59fb5a7",
   "metadata": {},
   "source": [
    "Good enough for me!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8139cf56",
   "metadata": {},
   "source": [
    "### Store the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c7278eee",
   "metadata": {},
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "features does not exist. Available: Sentiment, Text",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-32f4bda3c9f0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mModel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdfTrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mPredict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdfTest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/spark-3.1.1-bin-hadoop2.7/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    159\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m/spark-3.1.1-bin-hadoop2.7/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 335\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    336\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/spark-3.1.1-bin-hadoop2.7/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    330\u001b[0m         \"\"\"\n\u001b[1;32m    331\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: features does not exist. Available: Sentiment, Text"
     ]
    }
   ],
   "source": [
    "Model = Model.fit(dfTrain)\n",
    "Predict = Model.transform(dfTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf82d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = valDF.filter(valDF.label == valDF.prediction).count()/float(valDF.count())\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\n",
    "AUC = evaluator.evaluate(valDF)\n",
    "print(\"Accuracy: {}\\nAUC: {}\".format(accuracy,AUC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e23bec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Text = sc.textFile(\"./test.csv\")\n",
    "\n",
    "dataTest = ( \n",
    "    Text.map(lambda x: (x.split('\",\"')[5])) # get sentiment and text\n",
    "    .map(lambda x: (x[0],re.sub(r'@[^\\s]+','',x[1])))  #remove mentions\n",
    "    .map(lambda x: (x[0],re.sub(r\"\\S*http?:\\S*\",'',x[1])))  #remove urls\n",
    "    .map(lambda x: (x[0],re.sub(r'[^A-Za-z0-9 ]','',x[1])))  #remove special\n",
    "    .map(lambda x: (x[0],re.sub(r'[ ]{2,}',' ',x[1])))  #remove double+ spaces\n",
    "    )\n",
    "\n",
    "df = dataTest.toDF(['Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2cd3d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "Model = LogisticRegression(maxIter=300)\n",
    "pipeline = Pipeline(stages=[tokenizer,remover,TF,IDF,Model])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c41c99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d92f60d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
